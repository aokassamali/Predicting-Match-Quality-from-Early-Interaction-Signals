# Stage B Final Summary — Mutual Date Quality Ranking

## What Stage B is doing
**Goal:** Given a user in a speed-dating event (“wave”), rank the people they met so that the *top* of the list contains the highest-quality mutual outcomes.

- **Query / candidate set:** a single (wave, iid) pair, where the candidates are the set of partners `pid` that `iid` met in that wave.
- **Label (“date quality”):** `quality = min(like, like_o)` in \[0, 10\]. This captures *mutual* enthusiasm (both sides have to like it).
- **Relevance for NDCG:** `quality_bin` (binary relevance derived from quality; consistent across Scripts 12/13).
- **Why this matters:** In two-sided markets (dating), *ranking* is the product. You usually don’t care about perfectly predicting every interaction — you care about surfacing the best few fast.

## Data overview
- Avg candidates per query: **14.56**
- Avg tie ratio: **0.593** (many tied relevance values → tie-breaking matters)
- Pct queries with ≥1 very-high-quality option (≥8/10): **0.546**
- Mean count of ≥8/10 per query: **1.016**

## Final model
**Model:** LightGBM LambdaMART (Lambdarank)  
**Frozen config:**
```json
{
  "learning_rate": 0.03,
  "num_leaves": 31,
  "min_data_in_leaf": 150,
  "lambda_l2": 20.0,
  "max_depth": 8,
  "feature_fraction": 0.85,
  "bagging_fraction": 0.85,
  "bagging_freq": 1
}
```

## Evaluation protocol
- **Outer split:** by `wave` (to prevent event leakage).
- **Nested CV:** inner split selects hyperparameters by **INNER-VAL NDCG@5**, outer split reports mean±std on **OUTER-TEST**.
- **Primary metric:** **NDCG@5** (top-of-list quality).

## Results (Outer-TEST; mean ± std)
**Baselines**
- Random:
  - K=5  NDCG **0.4878 ± 0.0487**
  - K=10 NDCG **0.6156 ± 0.0534**
- Partner prior (pid mean quality from TRAIN only):
  - K=5  NDCG **0.4800 ± 0.0450**
  - K=10 NDCG **0.6084 ± 0.0524**

**Model**
- K=5  NDCG **0.5296 ± 0.0530**
- K=10 NDCG **0.6460 ± 0.0515**
- Recall@K (threshold metrics):
  - Recall@5 (quality ≥ 7): **0.6924**
  - Recall@5 (quality ≥ 8): **0.3169**

## Context: Stage A was intentionally a “pivot point”
Stage A tried to rank **mutual match** (binary outcome). It worked *somewhat* but the label was noisy / brittle across waves.

Best Stage A (core features; outer-test):
- K=5  NDCG **0.2920 ± 0.0464**
- K=10 NDCG **0.3879 ± 0.0467**

**Interpretation:** “Match” is a weaker proxy than “mutual satisfaction.” Stage B became the main deliverable because the supervision signal is cleaner and more aligned with product value.
## Feature importance (SHAP-style)

To make the model explainable for interviews/README, I computed **SHAP-style feature contributions** using LightGBM's `pred_contrib` on a single model trained on the full Stage B dataset (for interpretability only). I report **mean absolute contribution** across a random sample of rows.

> Interpretation note: these importances show which inputs the model *uses* to change the ranking score. They are **not causal**.

Role prefixes used in the plot/table:
- **V:** candidate/partner-side feature (`*_v`, i.e. `pid`)
- **U:** pairwise feature derived from both sides (e.g., `abs_age_diff`)

Top-10 features:

| rank | Feature (short) | Feature (raw) | Mean abs SHAP |
| --- | --- | --- | --- |
| 1 | V:Partner fun-ness | num__fun3_1_v | 0.135 |
| 2 | V:Excitement for event | num__exphappy_v | 0.08 |
| 3 | V:Same religion | num__imprelig_v | 0.077 |
| 4 | U:Absolute age difference | num__abs_age_diff | 0.062 |
| 5 | V:Same ethnicity | num__imprace_v | 0.062 |
| 6 | V:Shared interests | num__shar1_1_v | 0.055 |
| 7 | V:Partner ambitiousness | num__amb3_1_v | 0.051 |
| 8 | V:Age | num__age_v | 0.049 |
| 9 | V:Partner intelligence | num__intel1_1_v | 0.048 |
| 10 | V:Partner sincerity | num__sinc1_1_v | 0.044 |

Artifacts generated by `scripts/14_make_stage_b_figure.py`:
- `results/figures/stage_b_feature_importance_shap.png`
- `results/figures/stage_b_top10_features_shap.csv`
- `results/figures/stage_b_ndcg_comparison.png`
